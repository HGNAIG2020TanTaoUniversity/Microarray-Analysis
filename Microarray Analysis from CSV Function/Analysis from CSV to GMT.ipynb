{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required functions\n",
    "import GEOparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats.mstats import zscore\n",
    "import time\n",
    "#Time sleep to prevent crashes\n",
    "time.sleep(1)\n",
    "#Change this to your working directory\n",
    "WORKDIR = '/Users/MaayanLab/Desktop/Allie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define merge function for combining sample lists\n",
    "def merge(dict1, dict2): \n",
    "        res = {**dict1, **dict2} \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define characteristic direction function\n",
    "def chdir(data, sampleclass, genes, gamma=1., sort=True, calculate_sig=False, nnull=10, sig_only=False, norm_vector=True):\n",
    "        data.astype(float)\n",
    "        #sampleclass = np.array(map(int, sampleclass))\n",
    "        m_non0 = sampleclass != 0\n",
    "        m1 = sampleclass[m_non0] == 1\n",
    "        m2 = sampleclass[m_non0] == 2\n",
    "\n",
    "        data = data[:, m_non0]\n",
    "        data = zscore(data)\n",
    "\n",
    "        n1 = m1.sum() # number of controls\n",
    "        n2 = m2.sum() # number of experiments\n",
    "\n",
    "        meanvec = data[:,m2].mean(axis=1) - data[:,m1].mean(axis=1) \n",
    "\n",
    "        pca = PCA(n_components=None)\n",
    "        pca.fit(np.array(data.T))\n",
    "\n",
    "        cumsum = pca.explained_variance_ratio_\n",
    "        keepPC = len(cumsum[cumsum > 0.001])\n",
    "\n",
    "        v = pca.components_[0:keepPC].T\n",
    "        r = pca.transform(data.T)[:,0:keepPC]\n",
    "        dd = ( np.dot(r[m1].T,r[m1]) + np.dot(r[m2].T,r[m2]) ) / float(n1+n2-2)\n",
    "        sigma = np.mean(np.diag(dd))\n",
    "\n",
    "        shrunkMats = np.linalg.inv(gamma*dd + sigma*(1-gamma)*np.eye(keepPC))\n",
    "        b = np.dot(v, np.dot(np.dot(v.T, meanvec), shrunkMats))\n",
    "\n",
    "        if norm_vector:\n",
    "            b /= np.linalg.norm(b)\n",
    "\n",
    "        grouped = zip([abs(item) for item in b],b,genes)\n",
    "        if sort:\n",
    "            grouped = sorted(grouped,key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        if not calculate_sig: # return sorted b and genes.\n",
    "            res = [(item[1],item[2]) for item in grouped]\n",
    "            return res\n",
    "        else: # generate a null distribution of chdirs\n",
    "            nu = n1 + n2 - 2\n",
    "            y1 = np.random.multivariate_normal(np.zeros(keepPC), dd, nnull).T * np.sqrt(nu / chi2.rvs(nu,size=nnull))\n",
    "            y2 = np.random.multivariate_normal(np.zeros(keepPC), dd, nnull).T * np.sqrt(nu / chi2.rvs(nu,size=nnull))\n",
    "            y = y2 - y1\n",
    "\n",
    "            nullchdirs = []\n",
    "            for col in y.T:\n",
    "                bn = np.dot(np.dot(np.dot(v,shrunkMats), v.T), np.dot(col,v.T))\n",
    "                bn /= np.linalg.norm(bn)\n",
    "                bn = bn ** 2\n",
    "                bn.sort()\n",
    "                bn = bn[::-1] ## sort in decending order\n",
    "                nullchdirs.append(bn)\n",
    "\n",
    "            nullchdirs = np.array(nullchdirs).T\n",
    "            nullchdirs = nullchdirs.mean(axis=1)\n",
    "            b_s = b ** 2 \n",
    "            b_s.sort()\n",
    "            b_s = b_s[::-1] # sorted b in decending order\n",
    "            relerr = b_s / nullchdirs ## relative error\n",
    "            # ratio_to_null\n",
    "            ratios = np.cumsum(relerr)/np.sum(relerr)- np.linspace(1./len(meanvec),1,len(meanvec))\n",
    "            res = [(item[1],item[2], ratio) for item, ratio in zip(grouped, ratios)] \n",
    "            print('Number of significant genes: %s'%(np.argmax(ratios)+1))\n",
    "            if sig_only:\n",
    "                return res[0:np.argmax(ratios)+1]\n",
    "            else:\n",
    "                return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Microarray Analysis Pipeline\n",
    "def micro_analysis(accession_id, control_samples, treated_samples):\n",
    "    #Creating a dictionary of assigned control and treated samples \n",
    "      \n",
    "    control_samples = { i : 'control' for i in control_samples }\n",
    "    treated_samples = { i : 'treated' for i in treated_samples }\n",
    "    all_samples = merge(control_samples, treated_samples)\n",
    "    \n",
    "    #Parse the GEO data using the Accession ID\n",
    "    gse = GEOparse.get_GEO(geo=accession_id, destdir=\"./\")\n",
    "    #Create a list of samples to use in the development of the expression matrix\n",
    "    list_samples = list(all_samples.keys())\n",
    "    \n",
    "    #Visualization of expression matrix\n",
    "    pivoted_samples = gse.pivot_samples('VALUE')[list_samples]\n",
    "    pivoted_samples.head()\n",
    "    #Determine the total amount of probes used in the study\n",
    "    pivoted_samples_average = pivoted_samples.median(axis=1)\n",
    "    #Filtering out unexpressed probes\n",
    "    expression_threshold = pivoted_samples_average.quantile(0.3)\n",
    "    expressed_probes = pivoted_samples_average[pivoted_samples_average >= expression_threshold].index.tolist()\n",
    "    \n",
    "    #Redefine expression data using only the expressed probes\n",
    "    exprsdata = gse.pivot_samples(\"VALUE\").loc[expressed_probes]\n",
    "    exprsdata = exprsdata.T\n",
    "    #Deletes additional samples that aren't being analyzed\n",
    "    exprsdata = exprsdata[exprsdata.index.isin(list_samples)]\n",
    "    #Drop any probe columns where expression data is missing or negative\n",
    "    exprsdata.dropna(axis = 1)\n",
    "    \n",
    "    #Quantile normalization of data\n",
    "    rank_mean = exprsdata.stack().groupby(exprsdata.rank(method='first').stack().astype(int)).mean()\n",
    "    exprsdata.rank(method='min').stack().astype(int).map(rank_mean).unstack().dropna(axis=1)\n",
    "    #Making Dataframe of samples\n",
    "    samplesDf = pd.DataFrame.from_dict(all_samples, orient = 'index', columns = ['type'])\n",
    "    samplesDf.reset_index(inplace=True)\n",
    "    \n",
    "    #Transpose data matrix for sorting, index correlated to probe IDs\n",
    "    exprsdata = exprsdata.T\n",
    "    #Upload annotation file as dictionary\n",
    "    dict1 = {}\n",
    "    with open(WORKDIR + '/probe2gene.txt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            (platform, probe, symbol) = line.split()\n",
    "            dict1[probe] = symbol\n",
    "    #Reset index and replace with gene symbols, view as dataframe\n",
    "    exprsdata = pd.DataFrame(exprsdata)\n",
    "    exprsdata.index = exprsdata.index.astype(str, copy=False)\n",
    "    exprsdata['symbol'] = exprsdata.index.to_series().map(dict1)\n",
    "    exprsdata.reset_index(inplace=True)\n",
    "    data = exprsdata.set_index('symbol')\n",
    "    \n",
    "    #Drop probe id column\n",
    "    data = data.drop('ID_REF', axis=1)\n",
    "    #Drop rows that aren't associated with a particular gene symbol\n",
    "    data = data.reset_index().dropna().set_index('symbol')\n",
    "    \n",
    "    #Utilize warning statements\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    #Make sample classes, ensure that there is a distinction between control/treated samples\n",
    "    data_cd = {}\n",
    "    sample_classes = {}\n",
    "    sample_class = np.zeros(data.shape[1], dtype=np.int32)\n",
    "    sample_class[samplesDf['type'].values == 'control'] = 1\n",
    "    sample_class[samplesDf['type'].values == 'treated'] = 2\n",
    "    sample_classes = sample_class\n",
    "    \n",
    "    #CD results\n",
    "    cd_res = chdir(data.values, sample_classes, data.index, gamma=.5, sort=False, calculate_sig=False)\n",
    "    cd_coefs = np.array(list(map(lambda x: x[0], cd_res)))\n",
    "    srt_idx = np.abs(cd_coefs).argsort()[::-1]\n",
    "    cd_coefs = cd_coefs[srt_idx][:600]\n",
    "    sorted_DEGs = data.index[srt_idx][:600]\n",
    "    up_genes = dict(zip(sorted_DEGs[cd_coefs > 0], cd_coefs[cd_coefs > 0]))\n",
    "    dn_genes = dict(zip(sorted_DEGs[cd_coefs < 0], cd_coefs[cd_coefs < 0]))\n",
    "    data_cd['up'] = up_genes\n",
    "    data_cd['dn'] = dn_genes\n",
    "    \n",
    "    #Retrieve up and down gene sets\n",
    "    up_list = list(up_genes.keys())\n",
    "    dn_list = list(dn_genes.keys())\n",
    "    #Up genes and down genes\n",
    "    return up_list, dn_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read metadata file\n",
    "metadata = pd.read_csv(WORKDIR + '/example_metadata.csv')\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run analysis over studies in the metadata file, printing a list of the up and down genes\n",
    "up_data = {}\n",
    "dn_data = {}\n",
    "for i in range(0,len(metadata.index)):\n",
    "    accession_id = metadata.iloc[i]['GEO Accession Number']\n",
    "    control_samples = metadata.iloc[i]['Control Samples']\n",
    "    treated_samples = metadata.iloc[i]['Treated Samples']\n",
    "    cell_type = str(metadata.iloc[i]['Cell Type'])\n",
    "    phys = str(metadata.iloc[i]['Altered Condition'])\n",
    "    drug = str(metadata.iloc[i]['Drug Name'])\n",
    "    gene = str(metadata.iloc[i]['Name of the Perturbed Gene'])\n",
    "    gene_type = str(metadata.iloc[i]['Gene Alteration'])\n",
    "    platform = str(metadata.iloc[i]['GEO Platform'])\n",
    "    metadata_list = [platform, cell_type, phys, drug, gene, gene_type]\n",
    "    control_samples = control_samples.split(',')\n",
    "    control_samples = [c.strip(' ') for c in control_samples]\n",
    "    treated_samples = treated_samples.split(',')\n",
    "    treated_samples = [t.strip(' ') for t in treated_samples]\n",
    "    DEGs = micro_analysis(accession_id, control_samples, treated_samples)\n",
    "    up_genes, dn_genes = DEGs\n",
    "    up_data[accession_id + ' ' + str(metadata_list) + ' up'] = up_genes\n",
    "    dn_data[accession_id + str(metadata_list) + ' dn'] = dn_genes\n",
    "    print(up_data, dn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merged up and down data\n",
    "updn_data = merge(up_data, dn_data)\n",
    "#Extract labels from lists of up/down genes\n",
    "updn_terms = [k for k,v in updn_data.items()]\n",
    "#Extract gene sets\n",
    "updnval = [updn_data[k] for k in updn_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final version of the desired format of  labels\n",
    "final_terms = []\n",
    "for term in updn_terms:\n",
    "    accession = term.split('[')[0].strip()\n",
    "    platform = term.split('[')[1].split(', ')[0].replace(\"'\",'')\n",
    "    cell_type = term.split('[')[1].split(', ')[1].replace(\"'\",'')\n",
    "    physical_alt = term.split('[')[1].split(', ')[2].replace(\"'\",'')\n",
    "    chemical_alt = term.split('[')[1].split(', ')[3].replace(\"'\",'')\n",
    "    genetic_alt = term.split('[')[1].split(', ')[4].replace(\"'\",'')\n",
    "    genetic_alt_type = term.split('[')[1].split(', ')[5].split(']')[0].replace(\"'\",'')\n",
    "    effect = term.split(']')[0].split(', ')[6].replace(\"'\",'')\n",
    "    updown = term.split('] ')[1].replace(\"'\",'')\n",
    "#Customize which categories of metadata to include on the labels\n",
    "    final_terms.append(str([str(accession), str(effect), str(updown)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new dictionary with the updated labels\n",
    "micro_data = dict(zip(final_terms, updnval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload genes from autophagy geneshot search and sort by count*frac\n",
    "geneshot = pd.read_csv('geneshot_genes.tsv', sep = '\\t')\n",
    "geneshot['count*frac'] = [geneshot['Publication count'].iloc[i] * geneshot['Fraction of publications from total gene publication'].iloc[i] for i in range(0,len(geneshot))]\n",
    "geneshot = geneshot.sort_values(by=['count*frac'], ascending = False)\n",
    "geneshot_list = list(geneshot['Gene'])[:300]\n",
    "#Put geneshot results into dictionary\n",
    "geneshot_dict = {}\n",
    "geneshot_dict[str(['Geneshot Autophagy Search', 'nan', 'nan'])] = geneshot_list\n",
    "\n",
    "#Upload predicted genes from autophagy geneshot search and make dictionaries\n",
    "geneshot1 = pd.read_csv('autorif_predictions.tsv', sep = '\\t')\n",
    "geneshot_list1 = list(geneshot1['Gene'])\n",
    "geneshot_dict1 = {}\n",
    "geneshot_dict1[str(['Geneshot AutoRif Predictions', 'nan', 'nan'])] = geneshot_list1\n",
    "\n",
    "geneshot2 = pd.read_csv('generif_predictions.tsv', sep = '\\t')\n",
    "geneshot_list2 = list(geneshot2['Gene'])\n",
    "geneshot_dict2 = {}\n",
    "geneshot_dict2[str(['Geneshot GeneRif Predictions', 'nan', 'nan'])] = geneshot_list2\n",
    "\n",
    "geneshot3 = pd.read_csv('enrichr_predictions.tsv', sep = '\\t')\n",
    "geneshot_list3 = list(geneshot3['Gene'])\n",
    "geneshot_dict3 = {}\n",
    "geneshot_dict3[str(['Geneshot Enrichr Predictions', 'nan', 'nan'])] = geneshot_list3\n",
    "\n",
    "geneshot4 = pd.read_csv('tagger_predictions.tsv', sep = '\\t')\n",
    "geneshot_list4 = list(geneshot4['Gene'])\n",
    "geneshot_dict4 = {}\n",
    "geneshot_dict4[str(['Geneshot Tagger Predictions', 'nan', 'nan'])] = geneshot_list4\n",
    "\n",
    "geneshot5 = pd.read_csv('archs4_predictions.tsv', sep = '\\t')\n",
    "geneshot_list5 = list(geneshot5['Gene'])\n",
    "geneshot_dict5 = {}\n",
    "geneshot_dict5[str(['Geneshot ARCHS4 Predictions', 'nan', 'nan'])] = geneshot_list5\n",
    "\n",
    "#Merge all geneshot dictionaries\n",
    "geneshot01 = merge(geneshot_dict, geneshot_dict1)\n",
    "geneshot23 = merge(geneshot_dict2, geneshot_dict3)\n",
    "geneshot45 = merge(geneshot_dict4, geneshot_dict5)\n",
    "geneshot03 = merge(geneshot01, geneshot23)\n",
    "all_geneshot = merge(geneshot03, geneshot45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload 3 sets if DEG from most relevant biojupies studies\n",
    "biojupies1 = pd.read_csv(WORKDIR + '/biojupies_genes1.tsv', sep = '\\t')\n",
    "biojupies2 = pd.read_csv(WORKDIR + '/biojupies_genes2.tsv', sep = '\\t')\n",
    "biojupies3 = pd.read_csv(WORKDIR + '/biojupies_genes3.tsv', sep = '\\t')\n",
    "\n",
    "#Sort into up and down sets\n",
    "biojupies1_up = biojupies1[biojupies1['Up/Down'] == 'up']\n",
    "biojupies1_dn = biojupies1[biojupies1['Up/Down'] != 'up']\n",
    "biojupies2_up = biojupies2[biojupies2['Up/Down'] == 'up']\n",
    "biojupies2_dn = biojupies2[biojupies2['Up/Down'] != 'up']\n",
    "biojupies3_up = biojupies3[biojupies3['Up/Down'] == 'up']\n",
    "biojupies3_dn = biojupies3[biojupies3['Up/Down'] != 'up']\n",
    "\n",
    "#Make gene sets\n",
    "bio1_up_list = list(biojupies1_up['Gene'])\n",
    "bio1_dn_list = list(biojupies1_dn['Gene'])\n",
    "bio2_up_list = list(biojupies2_up['Gene'])\n",
    "bio2_dn_list = list(biojupies2_dn['Gene'])\n",
    "bio3_up_list = list(biojupies3_up['Gene'])\n",
    "bio3_dn_list = list(biojupies3_dn['Gene'])\n",
    "\n",
    "#Make dictionaries for each gene set\n",
    "bio1up = {}\n",
    "bio1up[str(['GSE89672', 'Activate', 'up'])] = bio1_up_list\n",
    "bio1dn = {}\n",
    "bio1dn[str(['GSE89672', 'Activate', 'dn'])] = bio1_dn_list\n",
    "bio2up = {}\n",
    "bio2up[str(['GSE100888', 'Inhibit', 'up'])] = bio2_up_list\n",
    "bio2dn = {}\n",
    "bio2dn[str(['GSE100888', 'Inhibit', 'dn'])] = bio2_dn_list\n",
    "bio3up = {}\n",
    "bio3up[str(['GSE72091', 'Activate', 'up'])] = bio3_up_list\n",
    "bio3dn = {}\n",
    "bio3dn[str(['GSE72091', 'Activate', 'dn'])] = bio3_dn_list\n",
    "\n",
    "#Merge all biojupies dictionaries\n",
    "bio1 = merge(bio1up, bio1dn)\n",
    "bio2 = merge(bio2up, bio2dn)\n",
    "bio3 = merge(bio3up, bio3dn)\n",
    "bio12 = merge(bio1, bio2)\n",
    "biojupies = merge(bio12, bio3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge biojupies and geneshot dictionaries\n",
    "bio_geneshot = merge(biojupies, all_geneshot)\n",
    "#Merge biojupies, geneshot, and microarray dictionaries\n",
    "biogsmicro_data = merge(bio_geneshot, micro_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse gene list from Harmonizome's json of GO Biological Process Annotation for autophagy\n",
    "url1 = 'https://amp.pharm.mssm.edu/Harmonizome/api/1.0/gene_set/autophagy/GO+Biological+Process+Annotations'\n",
    "page1 = re.get(url1)\n",
    "soup1 = str(BeautifulSoup(page1.text, 'html.parser'))\n",
    "data1 = json.loads(soup1)\n",
    "list1 = data1['associations']\n",
    "gene_list1 = []\n",
    "for i in range(1, (len(list1))):\n",
    "    dict1a = list1[i-1]\n",
    "    dict2a = list1[i]\n",
    "    dict3a = merge(dict1a, dict2a)\n",
    "    gene_list1.append(dict3a['gene']['symbol'])\n",
    "gene_dict1 = {}\n",
    "gene_dict1[str(['Gene Ontology', 'nan', 'nan'])] = gene_list1\n",
    "\n",
    "#Parse gene list from Harmonizome's json of Kegg Pathways for autophagy\n",
    "url2 = 'https://amp.pharm.mssm.edu/Harmonizome/api/1.0/gene_set/regulation+of+autophagy/KEGG+Pathways'\n",
    "page2 = re.get(url2)\n",
    "soup2 = str(BeautifulSoup(page2.text, 'html.parser'))\n",
    "data2 = json.loads(soup2)\n",
    "list2 = data2['associations']\n",
    "gene_list2 = []\n",
    "for i in range(1, (len(list2))):\n",
    "    dict1b = list2[i-1]\n",
    "    dict2b = list2[i]\n",
    "    dict3b = merge(dict1b, dict2b)\n",
    "    gene_list2.append(dict3b['gene']['symbol'])\n",
    "gene_dict2 = {}\n",
    "gene_dict2[str(['KEGG Pathways', 'nan', 'nan'])] = gene_list2\n",
    "\n",
    "#Parse gene list from Harmonizome's json of Wikipathways for autophagy\n",
    "url3 = 'https://amp.pharm.mssm.edu/Harmonizome/api/1.0/gene_set/Senescence+and+Autophagy%28Homo+sapiens%29/Wikipathways+Pathways'\n",
    "page3 = re.get(url3)\n",
    "soup3 = str(BeautifulSoup(page3.text, 'html.parser'))\n",
    "data3 = json.loads(soup3)\n",
    "list3 = data3['associations']\n",
    "gene_list3 = []\n",
    "for i in range(1, (len(list3))):\n",
    "    dict1c = list3[i-1]\n",
    "    dict2c = list3[i]\n",
    "    dict3c = merge(dict1c, dict2c)\n",
    "    gene_list3.append(dict3c['gene']['symbol'])\n",
    "gene_dict3 = {}\n",
    "gene_dict3[str(['Wikipathways', 'nan', 'nan'])] = gene_list3\n",
    "\n",
    "#Merge Harmonizome dictionaries\n",
    "gene_dict12 = merge(gene_dict1, gene_dict2)\n",
    "lit_dict = merge(gene_dict12, gene_dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload CREEDS-generated up/down gene lists from GSE41018\n",
    "rapamycin = pd.read_csv(WORKDIR + '/creeds_rapamycin.tsv', sep='\\t')\n",
    "rapamycin_up = rapamycin[rapamycin['Up/Down'] == 'up']\n",
    "rapamycin_dn = rapamycin[rapamycin['Up/Down'] != 'up']\n",
    "\n",
    "#Make rapamycin dictionaries\n",
    "rapamycin_up_list = list(rapamycin_up['Gene'])\n",
    "rapamycin_dict_up = {}\n",
    "rapamycin_dict_up[str(['CREEDS Rapamycin Search', 'nan', 'up'])] = rapamycin_up_list\n",
    "rapamycin_dn_list = list(rapamycin_dn['Gene'])\n",
    "rapamycin_dict_dn = {}\n",
    "rapamycin_dict_dn[str(['CREEDS Rapamycin Search', 'nan', 'dn'])] = rapamycin_dn_list\n",
    "#Merge rapamycin dictionaries\n",
    "rapamycin_dict = merge(rapamycin_dict_up, rapamycin_dict_dn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge biojupies, microarray, geneshot, and Harmonizome dictionaries\n",
    "almost_all_data = merge(biogsmicro_data, lit_dict)\n",
    "\n",
    "#Merge biojupies, microarray, geneshot, Harmonizome, and Rapamycin dictionaries\n",
    "all_data = merge(almost_all_data, rapamycin_dict)\n",
    "\n",
    "#Extract labels from dictionaries\n",
    "data_terms = [k for k,v in all_data.items()]\n",
    "data_vals = [all_data[k] for k in data_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary dataframe of data\n",
    "all_datadf = pd.DataFrame.from_dict(all_data, orient = 'index')\n",
    "all_datadf.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new labels for gmt file\n",
    "gmt_terms = []\n",
    "for term in data_terms:\n",
    "    accession = term.split('[')[1].split(', ')[0].replace(\"'\",'')\n",
    "    effect = term.split('[')[1].split(', ')[1].replace(\"'\",'')\n",
    "    updown = term.split(', ')[2].split(']')[0].replace(\"'\",'')\n",
    "#Customize which categories of metadata to include on the labels\n",
    "    gmt_terms.append(list([str(accession), str(effect), str(updown)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dictionary of desired metadata to merge with preliminary data df\n",
    "gmt_df = pd.DataFrame(gmt_terms, columns=[\"ID\", \"Effect\", \"Up/Down\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate dataframes, adding metadata and setting ID as index\n",
    "main_df = pd.concat([gmt_df, all_datadf], axis=1)\n",
    "main_df = main_df.drop('index', axis=1)\n",
    "main_df = main_df.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send dataframe to tsv file\n",
    "all_autophagy_file = (WORKDIR + '/autophagy_data.gmt')\n",
    "main_df.to_csv(all_autophagy_file, sep = '\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
