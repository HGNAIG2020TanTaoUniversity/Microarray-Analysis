{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing a series of packages used throughout the pipeline\n",
    "import GEOparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The series accession id for the study you are analyzing\n",
    "geo_accession_id = \"GSE54917\"\n",
    "#Establishing a working directory\n",
    "WORKDIR = '/Users/MaayanLab/Desktop/Allie'\n",
    "#Separating control and treated samples\n",
    "control_samples = ['GSM1326549', 'GSM1326550', 'GSM1326551']\n",
    "treated_samples = ['GSM1326552', 'GSM1326553', 'GSM1326554', 'GSM1326555']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary of assigned control and treated samples\n",
    "def merge(control_samples, treated_samples): \n",
    "    res = {**control_samples, **treated_samples} \n",
    "    return res \n",
    "      \n",
    "control_samples = { i : 'control' for i in control_samples }\n",
    "treated_samples = { i : 'treated' for i in treated_samples }\n",
    "all_samples = merge(control_samples, treated_samples) \n",
    "print(all_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the GEO data using the Accession ID\n",
    "gse = GEOparse.get_GEO(geo=geo_accession_id, destdir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of samples to use in the development of the expression matrix\n",
    "list_samples = list(all_samples.keys())\n",
    "list_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of expression matrix\n",
    "pivoted_samples = gse.pivot_samples('VALUE')[list_samples]\n",
    "pivoted_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the total amount of probes used in the study\n",
    "pivoted_samples_average = pivoted_samples.median(axis=1)\n",
    "print(\"Number of probes before filtering: \", len(pivoted_samples_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out unexpressed probes\n",
    "expression_threshold = pivoted_samples_average.quantile(0.3)\n",
    "expressed_probes = pivoted_samples_average[pivoted_samples_average >= expression_threshold].index.tolist()\n",
    "print(\"number of probes above threshold: \", len(expressed_probes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine expression data using only the expressed probes\n",
    "exprsdata = gse.pivot_samples(\"VALUE\").loc[expressed_probes]\n",
    "exprsdata = exprsdata.T\n",
    "#Deletes additional samples that aren't being analyzed\n",
    "exprsdata = exprsdata[exprsdata.index.isin(list_samples)]\n",
    "#Drop any probe columns where expression data is missing or negative\n",
    "exprsdata.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Quantile normalization of data\n",
    "rank_mean = exprsdata.stack().groupby(exprsdata.rank(method='first').stack().astype(int)).mean()\n",
    "exprsdata.rank(method='min').stack().astype(int).map(rank_mean).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute PCA\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(exprsdata)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Dataframe of samples to concatenate with principal components\n",
    "samplesDf = pd.DataFrame.from_dict(all_samples, orient = 'index', columns = ['type'])\n",
    "samplesDf.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Concatenate sample data with PCA data\n",
    "principalDf = pd.concat([samplesDf, principalDf], axis=1)\n",
    "principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize = (12,12))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_zlabel('Principal Component 3', fontsize = 15)\n",
    "ax.set_title('3 Component PCA', fontsize = 20)\n",
    "\n",
    "types = ('control', 'treated')\n",
    "colors = ['green', 'violet']\n",
    "for type, color in zip(types, colors):\n",
    "    indicesToKeep = principalDf['type'] == type\n",
    "    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1'], \n",
    "               principalDf.loc[indicesToKeep, 'principal component 2'], principalDf.loc[indicesToKeep, 'principal component 3'], c = color, s = 50)\n",
    "ax.legend(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate variance ratio\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Transpose data matrix for sorting, index correlated to probe IDs\n",
    "exprsdata = exprsdata.T\n",
    "exprsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sort expression matrix using 800 genes with greatest variance\n",
    "variances = np.var(exprsdata, axis=1)\n",
    "srt_idx = variances.argsort()[::-1]\n",
    "data_sub = exprsdata.iloc[srt_idx].iloc[:800]\n",
    "data_sub.index = data_sub.index.map(str)\n",
    "data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract probe ids from data\n",
    "probeids = list(data_sub.index)\n",
    "probeids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload annotation file as dictionary\n",
    "dict1 = {}\n",
    "with open(WORKDIR + 'probe2gene.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        (platform, probe, symbol) = line.split()\n",
    "        dict1[probe] = symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine how many ids are duplicates for gene symbols/unmatched\n",
    "len(set(probeids) - dict1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reset index and replace with gene symbols, view as dataframe\n",
    "exprsdata = pd.DataFrame(exprsdata)\n",
    "exprsdata['symbol'] = exprsdata.index.to_series().map(dict1)\n",
    "exprsdata.reset_index(inplace=True)\n",
    "data = exprsdata.set_index('symbol')\n",
    "#Drop probe id column\n",
    "data = data.drop('ID_REF', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Drop rows that aren't associated with a particular gene symbol\n",
    "data = data.reset_index().dropna().set_index('symbol')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardized data to a text file\n",
    "data_file = (WORKDIR + 'expression_matrix_top800_genes.txt')\n",
    "data.to_csv(data_file, sep='\\t')\n",
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Post expression matrix to clustergrammer\n",
    "import requests, json\n",
    "clustergrammer_url = 'http://amp.pharm.mssm.edu/clustergrammer/matrix_upload/'\n",
    "\n",
    "r = requests.post(clustergrammer_url, files={'file': open(data_file, 'rb')})\n",
    "link = r.text\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Display clustergram\n",
    "from IPython.display import IFrame\n",
    "display(IFrame(link, width=\"1000\", height=\"1000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages for characteristic direction and utilize warning statements\n",
    "import warnings\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats.mstats import zscore\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define characteristic direction function\n",
    "def chdir(data, sampleclass, genes, gamma=1., sort=True, calculate_sig=False, nnull=10, sig_only=False, norm_vector=True):\n",
    "    data.astype(float)\n",
    "    #sampleclass = np.array(map(int, sampleclass))\n",
    "    \n",
    "    m_non0 = sampleclass != 0\n",
    "    m1 = sampleclass[m_non0] == 1\n",
    "    m2 = sampleclass[m_non0] == 2\n",
    "    \n",
    "    data = data[:, m_non0]\n",
    "    data = zscore(data)\n",
    "    \n",
    "    n1 = m1.sum() # number of controls\n",
    "    n2 = m2.sum() # number of experiments\n",
    "    \n",
    "    meanvec = data[:,m2].mean(axis=1) - data[:,m1].mean(axis=1) \n",
    "    \n",
    "    pca = PCA(n_components=None)\n",
    "    pca.fit(np.array(data.T))\n",
    "    \n",
    "    cumsum = pca.explained_variance_ratio_\n",
    "    keepPC = len(cumsum[cumsum > 0.001])\n",
    "    \n",
    "    v = pca.components_[0:keepPC].T\n",
    "    r = pca.transform(data.T)[:,0:keepPC]\n",
    "    dd = ( np.dot(r[m1].T,r[m1]) + np.dot(r[m2].T,r[m2]) ) / float(n1+n2-2)\n",
    "    sigma = np.mean(np.diag(dd))\n",
    "    \n",
    "    shrunkMats = np.linalg.inv(gamma*dd + sigma*(1-gamma)*np.eye(keepPC))\n",
    "    b = np.dot(v, np.dot(np.dot(v.T, meanvec), shrunkMats))\n",
    "    \n",
    "    if norm_vector:\n",
    "        b /= np.linalg.norm(b)\n",
    "        \n",
    "    grouped = zip([abs(item) for item in b],b,genes)\n",
    "    if sort:\n",
    "        grouped = sorted(grouped,key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    if not calculate_sig: # return sorted b and genes.\n",
    "        res = [(item[1],item[2]) for item in grouped]\n",
    "        return res\n",
    "    else: # generate a null distribution of chdirs\n",
    "        nu = n1 + n2 - 2\n",
    "        y1 = np.random.multivariate_normal(np.zeros(keepPC), dd, nnull).T * np.sqrt(nu / chi2.rvs(nu,size=nnull))\n",
    "        y2 = np.random.multivariate_normal(np.zeros(keepPC), dd, nnull).T * np.sqrt(nu / chi2.rvs(nu,size=nnull))\n",
    "        y = y2 - y1\n",
    "        \n",
    "        nullchdirs = []\n",
    "        for col in y.T:\n",
    "            bn = np.dot(np.dot(np.dot(v,shrunkMats), v.T), np.dot(col,v.T))\n",
    "            bn /= np.linalg.norm(bn)\n",
    "            bn = bn ** 2\n",
    "            bn.sort()\n",
    "            bn = bn[::-1] ## sort in decending order\n",
    "            nullchdirs.append(bn)\n",
    "\n",
    "        nullchdirs = np.array(nullchdirs).T\n",
    "        nullchdirs = nullchdirs.mean(axis=1)\n",
    "        b_s = b ** 2 \n",
    "        b_s.sort()\n",
    "        b_s = b_s[::-1] # sorted b in decending order\n",
    "        relerr = b_s / nullchdirs ## relative error\n",
    "        # ratio_to_null\n",
    "        ratios = np.cumsum(relerr)/np.sum(relerr)- np.linspace(1./len(meanvec),1,len(meanvec))\n",
    "        res = [(item[1],item[2], ratio) for item, ratio in zip(grouped, ratios)] \n",
    "        print('Number of significant genes: %s'%(np.argmax(ratios)+1))\n",
    "        if sig_only:\n",
    "            return res[0:np.argmax(ratios)+1]\n",
    "        else:\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sample classes, ensure that there is a distinction between control/treated samples\n",
    "data_cd = {}\n",
    "\n",
    "sample_classes = {}\n",
    "sample_class = np.zeros(data.shape[1], dtype=np.int32)\n",
    "sample_class[samplesDf['type'].values == 'control'] = 1\n",
    "sample_class[samplesDf['type'].values == 'treated'] = 2\n",
    "sample_classes = sample_class\n",
    "\n",
    "print(sample_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CD results\n",
    "cd_res = chdir(data.values, sample_classes, data.index, gamma=.5, sort=False, calculate_sig=False)\n",
    "cd_coefs = np.array(list(map(lambda x: x[0], cd_res)))\n",
    "\n",
    "srt_idx = np.abs(cd_coefs).argsort()[::-1]\n",
    "cd_coefs = cd_coefs[srt_idx][:600]\n",
    "sorted_DEGs = data.index[srt_idx][:600]\n",
    "up_genes = dict(zip(sorted_DEGs[cd_coefs > 0], cd_coefs[cd_coefs > 0]))\n",
    "dn_genes = dict(zip(sorted_DEGs[cd_coefs < 0], cd_coefs[cd_coefs < 0]))\n",
    "data_cd['up'] = up_genes\n",
    "data_cd['dn'] = dn_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve up and down gene sets\n",
    "up_list = list(up_genes.keys())\n",
    "dn_list = list(dn_genes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Post data to Enrichr\n",
    "import json\n",
    "\n",
    "ENRICHR_URL = 'https://amp.pharm.mssm.edu/Enrichr'\n",
    "\n",
    "def _enrichr_add_list(genes, meta=''):\n",
    "    genes_str = '\\n'.join(genes)\n",
    "    payload = {\n",
    "        'list': (None, genes_str),\n",
    "        'description': (None, meta)\n",
    "    }\n",
    "    # POST genes to the /addList endpoint\n",
    "    response = requests.post(\"%s/addList\" % ENRICHR_URL, files=payload)\n",
    "    list_ids = json.loads(response.text)\n",
    "    return list_ids\n",
    "\n",
    "def enrichr_link(genes, meta=''):\n",
    "    list_ids = _enrichr_add_list(genes, meta)\n",
    "    shortId = list_ids['shortId']\n",
    "    link = '%s/enrich?dataset=%s' % (ENRICHR_URL, shortId)\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print links for further analysis\n",
    "for key, d in data_cd.items():\n",
    "    time.sleep(1)\n",
    "    genes = list(data_cd[key].keys())\n",
    "    genes = [str(g) for g in genes]\n",
    "    link = enrichr_link(genes, key)\n",
    "    print(key)\n",
    "    print(link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
